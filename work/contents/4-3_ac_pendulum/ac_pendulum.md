# ActorCritic内部設計

## 事前準備

1. 学習用パラメータをDictionaryとして設定する(ここではクラス化していない)

## 学習実行関数の定義

1. 環境モデルの生成(OpenAIGym)
2. 状態変数の形状を取得する
3. 行動変数の形状を取得する
4. Actorのインスタンスを作成
5. Criticのインスタンスを生成
6. ActorインスタンスとCriticインスタンスを使用してバッチTD学習を実行する

### バッチTD学習の定義

1. 学習結果の出力先ディレクトリを定義
2. 1で定義した出力先を作成する
3. ログファイルを生成する
    1. ログファイル名を定義する
    2. ログファイルのヘッダーを定義する
    3. 学習のmetricsを書き出すためのログファイルを生成する
    4. 学習パラメータを書き出すためのログファイルを生成する
    5. 学習パラメータの呼び出し
    6. 行動系列を保管するためのnamedtupleを生成する
4. 学習用パラメータとして設定したバッチ数分の繰り返し処理を行う
    1. 環境から初期状態を取得する
    2. 1stepの総スコアを計算するための変数を定義する
    3. StepLoop用の繰り返し処理を行う(TD誤差を計算する際に何ステップ先まで考慮するか)
        1. Actorから次の行動を取得する
        2. 1で取得した行動変数をone-hotベクトル化する
        3. 1の行動変数を環境へ渡し、次状態・報酬・終端状態・情報を取得する
        4. 3で取得した報酬にreward_clippingを行う
        5. 4でclippingした値を足し合わせ用の変数へ加算する
        6. 行動系列保管用のnamedtupleへ取得した状態・onehotベクトル化した行動変数・clippingした報酬を格納する
        7. 3で取得した状態を変数へ一時格納しておく
        8. step数が定義したバッチ数に達した場合にStepLoop処理を終了する
    4. StepLoop処理の最終ステップの状態をCriticに渡し、価値関数を計算する
    5. この１バッチ分でアドバンテージ関数、目標値、行動変数のonehotベクトル値を格納するための変数を定義する
    6. 行動系列を新しい要素からForLoop処理を実行する
        1. 行動系列のうちstateをCriticに渡し、状態価値関数を計算する
        2. 以下計算式にしたがって目標値を計算する
        3. アドバンテージ関数を１ステップTD誤差、複数ステップTD誤差として計算する
        4. 3で計算したアドバンテージ関数、目標値、行動変数のone-hotベクトルを１バッチの計算に限り保持する
        5. ActorとCriticそれぞれの損失関数を計算する
        6. １バッチ毎それぞれのスコア、Actorの損失関数、Criticの損失関数をリストへ格納する
        7. 学習のscore(clippingしたrewardの合計値)、Actorの損失関数、Criticの損失関数をログファイルへ書き出す
        8. 100ステップ毎にscore(clippingしたrewardの合計値)、Actorの損失関数、Criticの損失関数をコンソールへ出力する
    9. 指定した感覚毎にActorの重み係数を保存する

- 複数ステップ計算
$target = step.reward + \gamma * target$
- 1ステップ計算
$target = step.reward + \gamma * value_last$
$value_last = current_value$