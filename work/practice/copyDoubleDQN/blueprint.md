# Double-DQN

## Modelの流れ

1. 最初の状態変数を取得
2. 行動系列$[S_t,A_t,R_t,S_{t+1}]$をストックする
    1. 状態変数を取得（初期値の場合はランダム）
    2. epsilon-greedy法に基づき次の行動を決定する
        1. Greedyな行動の場合はMainネットワークに基づいて行動を決定する
    3. 1の行動で遷移する状態変数を環境から取得する
3. ランダムで行動系列を取得して、TD誤差を計算する
    $R_t+\gamma*max(Q_{t+1})$
    1. max(Q_{t+1})
    [Input]
    状態変数dim_state
    [Output]
    行動価値関数len（ある状態における各行動での行動価値関数）
    2. $R_t$は行動系列から取得する
    3. $\gamma$は定数
4. ある状態における各行動でのTD誤差を計算し、最大値以外を0としてMainNetの教師データとする
